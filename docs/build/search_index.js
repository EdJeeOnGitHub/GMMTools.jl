var documenterSearchIndex = {"docs":
[{"location":"tutorials/logit/#Estimating-a-logit-decision-model","page":"Logit tutorial","title":"Estimating a logit decision model","text":"","category":"section"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"This tutorial demonstrates how to use GMMTools.jl to estimate preference parameters in a simple binary choice model with a logit structure. ","category":"page"},{"location":"tutorials/logit/#Set-up","page":"Logit tutorial","title":"Set up","text":"","category":"section"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"A driver needs to go to work, and needs to choose between a short route with a toll and a longer, but free, route. The driver's utility is denominated in the currency (e.g. dollars). Relative utility between routes is a function of how much longer the long route takes to travel as well as any tolls a driver faces in the route. ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"We observe the decisions of many drivers. Our goal is to use this data to estimate underlying preference parameters of the driving population at large. ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"We use a field experiment helps us with this estimation. Approximately half of the agents are \"treated\" in an experiment where they face a fixed charge of using the shorter route. For the control group, both routes are free. Comparing the number of drivers who take the shorter route, conditional on distance, in treated and control groups teaches us about underlying preferences. ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Let d_1 be the short distance and d_2 be the long distance. Define p as the price of the toll and T_i whether an individual is treated. Define the utilities of the short and long distance, respectively as","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"U_1i = alpha d_1i +  T_i p + sigma epsilon_1i \nU_2i = alpha d_2i + sigma epsilon_2i","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"In the model, the epsilon shocks are Type I extreme-valued with variance parameter 1. A driver chooses the shorter route (Route 1), when U_1i  U_2i As a consequence of our assumptions, we know the probability an individual chooses the shorter route is","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"mathbfP(textShorter route_i) = mathbfPleft(frac-alpha d_2i - d_1i + T_i psigma  epsilon_2i - epsilon_1iright)","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Define epsilon_12_i = epsilon_2i - epsilon_1i, which is distributed according to the logistic distribution such that","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"epsilon_12 sim frace^epsilon_121 + e^epsilon_12","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Define d as the difference between the long and short route, d = d_2 - d_1. Therefore, the probability someone chooses the shorter route can be re-written as","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"mathbfP(textShorter route_i mid d_i T_i) = fracexpleft(frac-alpha d_i + T_i psigmaright)1 + expleft(frac-alpha d + T_i psigmaright)","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"The unknown model parameters we will estimate are alpha, the value of travel time, and sigma the logit variance parameter. ","category":"page"},{"location":"tutorials/logit/#Estimation-Strategy","page":"Logit tutorial","title":"Estimation Strategy","text":"","category":"section"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"We are presented with a data with N individuals, half are treated and half are not. We  observe the time and cost difference between their short and long routes as well as their choices. Our goal is to guess theta = (alpha sigma) such that a theoretical prediction of drivers' choices best matches our observed data. ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"To show the strength of GMMTools, we will choose two moments which are  opaque functions of the data directly. These will be the model-predicted mean of take-up in the treated and control groups respectively. Define g to be the expected values of take-up for the control and treated groups according to the model described above. ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"g(theta) = beginbmatrix\nfrac1N2sum_i mid T_i = 0 mathbfP(textShorter route_i mid d_i T_i)   \nfrac1N2sum_i mid T_i = 0 mathbfP(textShorter route_i mid d_i T_i) \nendbmatrix ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"hatg = beginbmatrix\nfrac1N_0sum_i mid T_i = 0 textShorter route_i  \nfrac1N_1sum_i mid T_i = 1 textShorter route_i\nendbmatrix","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Define the anal","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"We select hattheta to minimize ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":" g(hattheta) - hatg","category":"page"},{"location":"tutorials/logit/#Setting-up-the-estimation","page":"Logit tutorial","title":"Setting up the estimation","text":"","category":"section"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Load the packages we will be using for the analysis","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"using GMMTools\nusing Parameters\nusing UnPack\nusing GLM\nusing Random, Statistics, LinearAlgebra","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Define containers for the known parameters in the model (the inputs) and the unknown parameters in the model (which we will determine via calibration)","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"@with_kw struct KnownParams{T}\n  p::T\nend","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"@with_kw struct UnKnownParams{T}\n  α::T\n  σ::T\nend","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"For a single individual, find mathbfP(textShorter route_i mid d_i T_i). We censor values to avoid numerical errors.","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"function get_indiv_pred_takeup(\n  dist,\n  treated,\n  kp::KnownParams,\n  up::UnKnownParams;\n  max_diff = 200)\n\n  @unpack α, σ = up\n  @unpack p = kp\n  inside_exp = (-α * dist + treated * p) / σ\n  t = clamp(inside_exp, -max_diff, max_diff)\n  e = exp(t)\n  e / (1 + e)\nend","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"We use the function above to now calculate g(hattheta). We return a 1times 2 matrix, in accordance with the expectations of the GMMTools.jl API.   ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"function get_moments_model(up::UnKnownParams, data, kp::KnownParams)\n  @unpack treatments, distances, takeups = data\n  treated_obs = treatments .== 1\n  control_obs = treatments .== 0\n  # Get treated and control takeup means predicted by model\n  pred_takeup = get_indiv_pred_takeup.(data.distances, data.treatments, Ref(kp), Ref(up))\n\n  [mean(pred_takeup[control_obs]) mean(pred_takeup[treated_obs])]\nend","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"In the third step, we calculate hatg, the observed means of take-up between treated and control groups. We also return the variance-covariance matrix of hatg, which we will use later in estimation. ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"function get_moments_data(data)\n  # Get treated and control takeup means\n  # By omitting the intersection and including the (perfectly)\n  # collinear) vector of who is a control, coefficients represent\n  # group means.\n  m = lm(@formula(takeups ~ 0 + controls + treatments), data)\n  M = permutedims(coef(m))\n\n  # The covariance matrix is now easily estimated from the\n  # regression output\n  V = vcov(m)\n\n  M, V\nend","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Putting the above together, we estimate ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"g(hattheta) - hatg ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"function get_moments_diff(up::UnKnownParams, data, kp::KnownParams)\n  M_data, V_data = get_moments_data(data)\n  M_model = get_moments_model(up, data, kp)\n\n  M_model .- M_data\nend","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Finally, we define a small function to generate the data so that we can perform the estimation. We input a random number generator to this function for reproducibility. ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"function generate_data(;\n  N,\n  max_distance,\n  kp::KnownParams,\n  true_up::UnKnownParams, # Actually known, here\n  rng = nothing)\n\n  distances = rand(rng, N) .* max_distance\n  treatments = rand(rng, N).< .5\n  controls = treatments .== 0\n\n  takeup_probs = get_indiv_pred_takeup.(distances, treatments, Ref(kp), Ref(true_up))\n  takeups = rand(rng, N) .< takeup_probs\n\n  (; distances, treatments, controls, takeups)\nend","category":"page"},{"location":"tutorials/logit/#Performing-the-estimation","page":"Logit tutorial","title":"Performing the estimation","text":"","category":"section"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"First, generate data according to the true parameters. ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"rng = MersenneTwister(123)\n\nkp = KnownParams(p = 10.0)\n\ntrue_up = UnKnownParams(α = 1.5, σ = 10.0)\n\ndata = generate_data(;\n  N = 50000,\n  kp,\n  max_distance = 20,\n  true_up, \n  rng)","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Now we finally use the GMMTools.jl library to estimate our unknown parameters. To do this we use the function run_estimation. run_estimation takes the following arguments:","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"momfn: The moment function, used in the form momfn(theta, data) where theta is a vector of parameters to be estimated.\ndata: The object passed to momfn\ntheta0: Matrix representing starting guesses of theta. It is of size main_n_start_pts x n_params where main_n_start_pts is taken from the dictionary containing gmm options. n_params is the length of theta.\ntheta0_boot: Akin to theta0, but contains starting values for each bootstrapping iteration. It is of size (boot_n_start_pts x boot_n_runs) x n_params\ntheta_lower: Vector of lower bounds (default is -Inf)\ntheta_upper: vector of upper bounds (default is +Inf)","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"First, we define momfn according to the requirements above","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"moment_fun = let kp = kp # Capture the known parameters\n  (θ, data) -> begin\n    up = UnKnownParams(α = θ[1], σ = θ[2])\n    get_moments_diff(up, data, kp)\n  end\nend","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Confirm that the moment function returns the correct 1 times 2 matrix","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"moment_fun([1, 2], data)","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Next, we define the GMM options that will control details of our estimation","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"output_dir = mktempdir()\ngmm_options = Dict{String, Any}(\n  # Do not run the estimation in parallel\n  \"main_run_parallel\" => false,\n\n  # Use a minimum distance estimator, rather than a \n  # GMM estimator. \n  \"estimator\" => \"cmd\",\n\n  # Re-run the estimation when we calculate standard\n  # errors, rather than re-use original parameter \n  # estimates\n  \"var_boot\" => \"slow\",\n  \"boot_n_runs\" => 5,\n\n  # Folder to store intermediate output \n  \"rootpath_output\" => output_dir,  \n\n  # Write intermediate parameter estimation\n  # to a file\n  \"main_write_results_to_file\" => true,\n\n  # Do not write the bootstrapping to file\n  \"boot_write_results_to_file\" => false,\n\n  # Print out progress in estimation of θ\n  \"show_progress\" => true,\n  \n  # Do not show progress for bootstrapping\n  \"boot_show_progress\" => false,\n\n  # Overwrite results\n  \"main_overwrite_runs\" => 10, ## 10=overwrite everything\n  \"boot_overwrite_runs\" => 10, ## 10=overwrite everything\n\n  # Throw errors during estimation of θ\n  \"main_throw_errors\" => true,\n\n  # Do not throw errors during bootstrapping\n  \"boot_throw_errors\" => false\n)","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Finally we are ready to run the estimation using the run_estimation function from GMMTools.jl","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"# Initial guess of θ\nθ_0 = [1.5 10.0]\nres = run_estimation(;\n  momfn = moment_fun,\n  data = data,\n  theta0 = θ_0,\n  theta_lower = [0.0, 0.0],\n  theta_upper = [Inf, Inf],\n  # Use the identity matrix to weight\n  omega = I,\n  gmm_options)","category":"page"},{"location":"tutorials/logit/#Inspecting-the-output","page":"Logit tutorial","title":"Inspecting the output","text":"","category":"section"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"The output res gives two objects, a DataFrame storing the minimum distance, best parameters, etc. and a Dict showing errors which may have occurred in the estimation.","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"res_df, res_dict = res","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Inspecting res_df.param_1 and res_df.param_2, we get the values ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"hatalpha = 1510 \nhatsigma = 9972","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"The optimizer was able to get close to the true values of alpha = 15 and sigma = 10. ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Inspecting res_dict shows there were no errors and no non-convergences during estimation. ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"These the results that exist in the Julia session. But GMMTools.jl also saves results to files. Let's inspect these individually. They are stored in output_dir, which we passed to the gmm_options dictionary above.","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"files = readdir(output_dir)\nreadshow(f) = println(read(joinpath(output_dir, f), String))","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"3-element Vector{String}:\n \"estimation_flags.json\"\n \"estimation_parameters.json\"\n \"estimation_results_df.csv\"","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"readshow(\"estimation_flags.json\")","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"{\n    \"n_success\": 1,\n    \"n_errors\": 0,\n    \"n_not_converged\": 0\n}","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"This corresponds to res_d","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"readshow(\"estimation_parameters.json\")","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"{\n    \"gmm_options\": {\n        \"rootpath_output\": \"/tmp/jl_I6JkzR\",\n        \"use_unconverged_results\": false,\n        \"var_boot\": \"slow\",\n        \"boot_maxIter\": 1000,\n        \"main_show_theta\": false,\n        \"main_throw_errors\": true,\n        \"main_write_results_to_file\": true,\n        \"boot_overwrite_runs\": 10,\n        \"main_maxIter\": 1000,\n        \"boot_throw_errors\": false,\n        \"estimator\": \"cmd\",\n        \"n_params\": 2,\n        \"main_maxTime\": null,\n        \"n_moms_full\": 2,\n        \"boot_maxTime\": null,\n        \"boot_show_progress\": false,\n        \"main_show_trace\": false,\n        \"boot_n_runs\": 5,\n        \"boot_write_results_to_file\": true,\n        \"show_progress\": true,\n        \"var_asy\": true,\n        \"boot_run_parallel\": false,\n        \"2step\": false,\n        \"main_n_initial_cond\": 1,\n        \"normalize_weight_matrix\": false,\n        \"boot_show_theta\": false,\n        \"main_run_parallel\": false,\n        \"boot_show_trace\": false,\n        \"main_overwrite_runs\": 10,\n        \"n_moms\": 2,\n        \"n_observations\": 1,\n        \"param_names\": null\n    },\n    \"W\": [\n        [\n            1.0,\n            0.0\n        ],\n        [\n            0.0,\n            1.0\n        ]\n    ],\n    \"theta0\": [\n        [\n            1.5\n        ],\n        [\n            10.0\n        ]\n    ],\n    \"main_n_initial_cond\": 1,\n    \"theta_fix\": null,\n    \"n_moms\": 2,\n    \"n_params\": 2,\n    \"theta_upper\": [\n        null,\n        null\n    ],\n    \"theta_lower\": [\n        0.0,\n        0.0\n    ],\n    \"n_moms_full\": 2,\n    \"moms_subset\": null,\n    \"omega\": {\n        \"λ\": true\n    },\n    \"n_observations\": 1\n}","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"This corresponds to the full set of inputs to the key function run_estimation. Reading this file back in will let us fully reproduce our original estimation procedure. ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"readshow(\"estimation_results_df.csv\")","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"obj_vals,opt_converged,opt_error,opt_error_message,opt_runtime,param_1,param_2,run_idx,is_optimum\n1.277058133722624e-15,true,false,,0.498569576,1.5101382430140904,9.971757744026124,1,true","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"This corresponds to res_df, which holds the parameter estimates. ","category":"page"},{"location":"tutorials/logit/#Fine-tuning-the-optimization","page":"Logit tutorial","title":"Fine-tuning the optimization","text":"","category":"section"},{"location":"tutorials/logit/#Using-GMM-estimation","page":"Logit tutorial","title":"Using GMM estimation","text":"","category":"section"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Here we perform a one-step GMM estimation, equivalent to solving","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"min_theta (hatg - g(hattheta)) Omega^-1 (hatg - g(hattheta)","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Where Omega represents the variance-covariance matrix of hatg - g(hattheta). For our purposes, we use the variance-covariance matrix of hatg, calculated with get_moments_data","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"_, V = get_moments_data(data)\n\ngmm_options_one_step = let \n  t = copy(gmm_options)\n  t[\"estimator\"] = \"gmm1step\"\n  t\nend\n\nres_one_step = run_estimation(;\n  momfn = moment_fun,\n  data = data,\n  theta0 = θ_0,\n  theta_lower = [0.0, 0.0],\n  theta_upper = [Inf, Inf],\n  # Use the identity matrix to weight\n  omega = V,\n  gmm_options = gmm_options_one_step)\n\nres_one_step_df, res_one_step_d = res_one_step","category":"page"},{"location":"tutorials/logit/#Multiple-initial-conditions","page":"Logit tutorial","title":"Multiple initial conditions","text":"","category":"section"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"Often, the function we are minimizing has many local minima. To ensure we are finding the global minimum, is is helpful to use random initial conditions. We do this with the random_initial_conditions function. ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"θ_0 = [1.5 10.0]\n\nθ_0s_random = random_initial_conditions(θ_0, [0, 0], [Inf, Inf], 100)\n\nres_random = run_estimation(;\n  momfn = moment_fun,\n  data = data,\n  theta0 = θ_0s_random,\n  theta_lower = [0.0, 0.0],\n  theta_upper = [Inf, Inf],\n  # Use the identity matrix to weight\n  omega = I,\n  gmm_options)\n\nres_random_df, res_random_d = res_random","category":"page"},{"location":"tutorials/logit/#Running-in-parallel-with-multiple-initial-conditions","page":"Logit tutorial","title":"Running in parallel with multiple initial conditions","text":"","category":"section"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"When we optimize with various initial conditions, these processes are entirely independent and can thus occur in parallel. ","category":"page"},{"location":"tutorials/logit/","page":"Logit tutorial","title":"Logit tutorial","text":"θ_0 = [1.5 10.0]\n\nθ_0s_random = random_initial_conditions(θ_0, [0, 0], [Inf, Inf], 100)\n\ngmm_options_parallel = let \n  t = copy(gmm_options)\n  t[\"main_run_parallel\"] = true\n  t\nend\n\nres_parallel = run_estimation(;\n  momfn = moment_fun,\n  data = data,\n  theta0 = θ_0s_random,\n  theta_lower = [0.0, 0.0],\n  theta_upper = [Inf, Inf],\n  # Use the identity matrix to weight\n  omega = I,\n  gmm_options)\n\nres_parallel_df, res_parallel_d = res_parallel","category":"page"},{"location":"#GMMTools.jl","page":"Introduction","title":"GMMTools.jl","text":"","category":"section"}]
}
